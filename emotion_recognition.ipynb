{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598165140600",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv2D,MaxPool2D,Dropout,Flatten,LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 7\n",
    "CLASSES = ['angry','disgust','fear','happy','neutral','sad','surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(16,kernel_size=(3,3),activation='relu',input_shape=(64,64,3)))\n",
    "model.add(Conv2D(32,kernel_size=(3,3),activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(32,kernel_size=(3,3),activation='relu'))\n",
    "model.add(Conv2D(64,kernel_size=(3,3),activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 28709 images belonging to 7 classes.\nFound 7178 images belonging to 7 classes.\n"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\n",
    "training_set = train_datagen.flow_from_directory('emotion/train',target_size = (64, 64),batch_size = 32)\n",
    "\n",
    "test_datagen = ImageDataGenerator()\n",
    "test_set = test_datagen.flow_from_directory(directory='emotion/test',target_size = (64, 64), batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/25\n898/898 [==============================] - 257s 286ms/step - loss: 0.6123 - accuracy: 0.8527 - val_loss: 0.3327 - val_accuracy: 0.8606\nEpoch 2/25\n898/898 [==============================] - 273s 304ms/step - loss: 0.3741 - accuracy: 0.8599 - val_loss: 0.3868 - val_accuracy: 0.8650\nEpoch 3/25\n898/898 [==============================] - 328s 365ms/step - loss: 0.3628 - accuracy: 0.8624 - val_loss: 0.3880 - val_accuracy: 0.8716\nEpoch 4/25\n898/898 [==============================] - 338s 377ms/step - loss: 0.3532 - accuracy: 0.8649 - val_loss: 0.3350 - val_accuracy: 0.8728\nEpoch 5/25\n898/898 [==============================] - 299s 333ms/step - loss: 0.3421 - accuracy: 0.8682 - val_loss: 0.3373 - val_accuracy: 0.8749\nEpoch 6/25\n898/898 [==============================] - 311s 346ms/step - loss: 0.3346 - accuracy: 0.8705 - val_loss: 0.2318 - val_accuracy: 0.8774\nEpoch 7/25\n898/898 [==============================] - 295s 329ms/step - loss: 0.3277 - accuracy: 0.8724 - val_loss: 0.2993 - val_accuracy: 0.8814\nEpoch 8/25\n898/898 [==============================] - 309s 344ms/step - loss: 0.3250 - accuracy: 0.8741 - val_loss: 0.3102 - val_accuracy: 0.8799\nEpoch 9/25\n898/898 [==============================] - 304s 339ms/step - loss: 0.3191 - accuracy: 0.8749 - val_loss: 0.3307 - val_accuracy: 0.8799\nEpoch 10/25\n898/898 [==============================] - 311s 347ms/step - loss: 0.3154 - accuracy: 0.8767 - val_loss: 0.2527 - val_accuracy: 0.8820\nEpoch 11/25\n898/898 [==============================] - 265s 295ms/step - loss: 0.3119 - accuracy: 0.8773 - val_loss: 0.3309 - val_accuracy: 0.8846\nEpoch 12/25\n898/898 [==============================] - 265s 295ms/step - loss: 0.3094 - accuracy: 0.8776 - val_loss: 0.2509 - val_accuracy: 0.8842\nEpoch 13/25\n898/898 [==============================] - 265s 295ms/step - loss: 0.3063 - accuracy: 0.8788 - val_loss: 0.2136 - val_accuracy: 0.8820\nEpoch 14/25\n898/898 [==============================] - 268s 298ms/step - loss: 0.3038 - accuracy: 0.8795 - val_loss: 0.3149 - val_accuracy: 0.8862\nEpoch 15/25\n898/898 [==============================] - 265s 296ms/step - loss: 0.3002 - accuracy: 0.8810 - val_loss: 0.2719 - val_accuracy: 0.8876\nEpoch 16/25\n898/898 [==============================] - 267s 298ms/step - loss: 0.3009 - accuracy: 0.8810 - val_loss: 0.3222 - val_accuracy: 0.8835\nEpoch 17/25\n898/898 [==============================] - 268s 298ms/step - loss: 0.3007 - accuracy: 0.8804 - val_loss: 0.3475 - val_accuracy: 0.8869\nEpoch 18/25\n898/898 [==============================] - 268s 299ms/step - loss: 0.2965 - accuracy: 0.8817 - val_loss: 0.2752 - val_accuracy: 0.8859\nEpoch 19/25\n898/898 [==============================] - 252s 281ms/step - loss: 0.2983 - accuracy: 0.8818 - val_loss: 0.2177 - val_accuracy: 0.8877\nEpoch 20/25\n898/898 [==============================] - 268s 299ms/step - loss: 0.2951 - accuracy: 0.8825 - val_loss: 0.2875 - val_accuracy: 0.8868\nEpoch 21/25\n898/898 [==============================] - 247s 275ms/step - loss: 0.2949 - accuracy: 0.8825 - val_loss: 0.3521 - val_accuracy: 0.8886\nEpoch 22/25\n898/898 [==============================] - 248s 276ms/step - loss: 0.2944 - accuracy: 0.8830 - val_loss: 0.2241 - val_accuracy: 0.8869\nEpoch 23/25\n898/898 [==============================] - 243s 271ms/step - loss: 0.2946 - accuracy: 0.8823 - val_loss: 0.3618 - val_accuracy: 0.8904\nEpoch 24/25\n898/898 [==============================] - 243s 271ms/step - loss: 0.2928 - accuracy: 0.8828 - val_loss: 0.2711 - val_accuracy: 0.8863\nEpoch 25/25\n898/898 [==============================] - 243s 271ms/step - loss: 0.2930 - accuracy: 0.8827 - val_loss: 0.2355 - val_accuracy: 0.8898\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x1c43afae198>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "model.fit(training_set,validation_data=test_set,epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "recogniser = keras.models.load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_9 (Conv2D)            (None, 62, 62, 16)        448       \n_________________________________________________________________\nconv2d_10 (Conv2D)           (None, 60, 60, 32)        4640      \n_________________________________________________________________\nmax_pooling2d_5 (MaxPooling2 (None, 30, 30, 32)        0         \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 30, 30, 32)        0         \n_________________________________________________________________\nconv2d_11 (Conv2D)           (None, 28, 28, 32)        9248      \n_________________________________________________________________\nconv2d_12 (Conv2D)           (None, 26, 26, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_6 (MaxPooling2 (None, 13, 13, 64)        0         \n_________________________________________________________________\ndropout_8 (Dropout)          (None, 13, 13, 64)        0         \n_________________________________________________________________\nflatten_3 (Flatten)          (None, 10816)             0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 256)               2769152   \n_________________________________________________________________\ndropout_9 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 7)                 1799      \n=================================================================\nTotal params: 2,803,783\nTrainable params: 2,803,783\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "recogniser.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def face(path):\n",
    "    face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.resize(img,(512,512))\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_detector.detectMultiScale(gray,1.1,4)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),3)\n",
    "\n",
    "    img= gray[y-32:y+h+32,x-32:x+w+32]\n",
    "    cv2.imwrite('image.png',img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for recognising people in front of camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "_ , frame = cap.read()\n",
    "cv2.imwrite('image.png',frame)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "face('image.png')\n",
    "# cv2.imshow('image',img)\n",
    "# cv2.waitKey()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = keras.preprocessing.image.load_img('image.png',target_size=[64,64,3])\n",
    "image=keras.preprocessing.image.img_to_array(image)\n",
    "image=np.expand_dims(image,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.20050837, 0.0028386 , 0.14539558, 0.00728727, 0.34907788,\n        0.29915527, 0.02225633]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "result = recogniser.predict(image)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'neutral'"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "max_prob_index = np.argmax(result)\n",
    "CLASSES[max_prob_index]\n"
   ]
  }
 ]
}